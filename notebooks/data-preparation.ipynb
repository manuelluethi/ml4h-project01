{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d886b04-868a-46d1-9b7e-a7f4945fb1af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Data preparation\n",
    "This notebook performs some basic data preparation. It saves a transformed\n",
    "version of the time series data in the folder \"data\" contained in the root\n",
    "directory. We also copy the outcome data to the same folder. Later on, we\n",
    "only work with data contained in there.\n",
    "\n",
    "## Tasks performed in this notebook\n",
    "* Read raw data\n",
    "* Transform to time-series data per individual in long format\n",
    "* Normalize time-points to hourly values\n",
    "* Save time-series data in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a15a52-d5ec-4589-97db-11991e5a67fc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974e119-62bf-42e6-b08c-2aabbfc50d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important directories\n",
    "# We first obtain the root directory. The way this is done depends on whether\n",
    "# we run the python notebook or the associated .py file as a script.\n",
    "# Hence, we use a helper function\n",
    "def getRootFolder():\n",
    "    if '__file__' in globals():\n",
    "        projectRoot = os.path.dirname(\n",
    "            os.path.dirname(\n",
    "                os.path.abspath(__file__)\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        projectRoot = os.path.dirname(os.getcwd())\n",
    "    return projectRoot\n",
    "ROOT_DIR = getRootFolder()\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data')\n",
    "LOG_DIR = os.path.join(ROOT_DIR, 'logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d67d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logger = logging.getLogger('data-preparation-logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if logger.hasHandlers():\n",
    "    # If the logger already has handlers, remove them\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "\n",
    "if not logger.hasHandlers():\n",
    "    # Create a directory for logs if it doesn't exist\n",
    "    try:\n",
    "        os.mkdir(LOG_DIR)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    # Create a file handler\n",
    "    log_file = os.path.join(LOG_DIR, 'data-preparation.log')\n",
    "    fh = logging.FileHandler(log_file, mode='w')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Create a console handler\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.CRITICAL)\n",
    "\n",
    "    # Create a formatter and set it for both handlers\n",
    "    log_format = '%(asctime)s - %(name)s - %(levelname)s:\\n\\t\\t%(message)s'\n",
    "    formatter = logging.Formatter(log_format)\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "\n",
    "    # Add the handlers to the logger\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951bc19-6012-4922-ac31-dc376fc1ddee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Preperations:\n",
    "# Boolean determining whether the script is run on the full data or\n",
    "# on a subset residing in subfolders with suffix -test\n",
    "# If the testdata doesn't exist, it will be created.\n",
    "# This is for debugging purposes only. The test data created is platform\n",
    "# dependent because of the use of os.scandir(), hence the results\n",
    "# are not reproducible.\n",
    "test = True\n",
    "\n",
    "# A list of all the time series variables to consider\n",
    "LIST_VARIABLE_TS = [\n",
    "    'ALP', 'ALT', 'AST', 'Albumin', 'BUN', 'Bilirubin',\n",
    "    'Cholesterol', 'Creatinine', 'DiasABP', 'FiO2', 'GCS', 'Glucose',\n",
    "    'HCO3', 'HCT', 'HR', 'ICUType', 'K', 'Lactate', 'MAP', 'MechVent',\n",
    "    'Mg', 'NIDiasABP', 'NIMAP', 'NISysABP', 'Na', 'PaCO2', 'PaO2', 'Platelets',\n",
    "    'RespRate', 'SaO2', 'SysABP', 'Temp', 'TroponinI', 'TroponinT', 'Urine',\n",
    "    'WBC', 'pH'\n",
    "]\n",
    "\n",
    "# A list of static variables\n",
    "LIST_VARIABLE_STATIC = [\n",
    "    'Age', 'Gender', 'Height', 'Weight'\n",
    "]\n",
    "\n",
    "# A list of keys:\n",
    "# RecordID indexes the individual times series\n",
    "# Hour is the time stamp to be used -- note that we will have to normalize\n",
    "# the variable Time which is in format hh:mm after admission to hourly\n",
    "# variables\n",
    "LIST_VARIABLE_KEYS = [\n",
    "    'RecordID', 'Hour'\n",
    "]\n",
    "\n",
    "# A list of all variables, including time series and static variables\n",
    "LIST_VARIABLES = LIST_VARIABLE_KEYS + LIST_VARIABLE_STATIC + LIST_VARIABLE_TS\n",
    "\n",
    "# fix types for the different variables\n",
    "TYPE_STRING = ['RecordID']\n",
    "TYPE_INTEGER = ['Hour']\n",
    "TYPE_CATEGORICAL = ['Gender']\n",
    "TYPE_FLOAT = (LIST_VARIABLE_TS + LIST_VARIABLE_STATIC).remove('Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f15cb-b761-4186-a5a4-c36c36360f07",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Fix path for data\n",
    "ORIG_DATA_PATH = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    os.path.join(\n",
    "        \"predicting-mortality-of-icu-patients-the-\"\n",
    "        \"physionetcomputing-in-cardiology-challenge-2012-1.0.0\",\n",
    "        \"predicting-mortality-of-icu-patients-the-physionetcomputing\"\n",
    "        \"-in-cardiology-challenge-2012-1.0.0\"\n",
    "    )\n",
    ")\n",
    "ORIG_DATA_TRAINING = os.path.join(ORIG_DATA_PATH, \"set-a\")\n",
    "ORIG_DATA_VALIDATION = os.path.join(ORIG_DATA_PATH, \"set-b\")\n",
    "ORIG_DATA_TESTING = os.path.join(ORIG_DATA_PATH, \"set-c\")\n",
    "rawDataPaths = [\n",
    "    ORIG_DATA_TRAINING,\n",
    "    ORIG_DATA_VALIDATION,\n",
    "    ORIG_DATA_TESTING\n",
    "]\n",
    "\n",
    "# if we want to run the code on a test subset, generate test data\n",
    "if test:\n",
    "    for rawDataPath in rawDataPaths:\n",
    "        try:\n",
    "            newFolder = rawDataPath + \"-test\"\n",
    "            os.mkdir(newFolder)\n",
    "            i = 0\n",
    "            for entry in os.scandir(rawDataPath):\n",
    "                if i < 100 and entry.name.endswith(\".txt\"):\n",
    "                    i += 1\n",
    "                    newPath = os.path.join(newFolder, entry.name)\n",
    "                    shutil.copy(entry.path, newPath)\n",
    "        except FileExistsError:\n",
    "            logger.info(\n",
    "                \"Test data folder \"\n",
    "                f\"{os.path.basename(os.path.normpath(rawDataPath))}-test \"\n",
    "                \"already exists. Using existing data.\"\n",
    "            )\n",
    "            pass\n",
    "    rawDataPaths = [p + \"-test\" for p in rawDataPaths]\n",
    "\n",
    "# Outcomes are already in one .txt-file (comma separated) per set. We save\n",
    "# the file names\n",
    "ORIG_OUTCOME_TRAINING = os.path.join(ORIG_DATA_PATH, \"Outcomes-a.txt\")\n",
    "ORIG_OUTCOME_VALIDATION = os.path.join(ORIG_DATA_PATH, \"Outcomes-b.txt\")\n",
    "ORIG_OUTCOME_TESTING = os.path.join(ORIG_DATA_PATH, \"Outcomes-c.txt\")\n",
    "rawOutcomesPaths = [\n",
    "    ORIG_OUTCOME_TRAINING,\n",
    "    ORIG_OUTCOME_VALIDATION,\n",
    "    ORIG_OUTCOME_TESTING\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the functions used for the data transformation\n",
    "\n",
    "\n",
    "# Constructor for the static dictionary:\n",
    "# We produce a long format data frame per patient, where the static variables\n",
    "# are treated as constants. Hence we keep track of them while producing the\n",
    "# data frame and fill them in the end.\n",
    "# - keys are the names of the static variables\n",
    "# - initialized with values pd.NA\n",
    "# Input:    - None\n",
    "# Output:   - a dictionary with keys as defined in LIST_VARIABLE_STATIC\n",
    "def initializeStaticDict():\n",
    "    staticDict = {key: pd.NA for key in LIST_VARIABLE_STATIC}\n",
    "    return staticDict\n",
    "\n",
    "\n",
    "# Constructor for the patient data frame\n",
    "# Initializes an empty data frame with a column per variable as defined before\n",
    "# Input:    - None\n",
    "# Output:   - a pandas.DataFrame with columns according to LIST_VARIABLES,\n",
    "#             i.e., the list of variables defined above.\n",
    "def initializeDataFrame():\n",
    "    # Create an empty DataFrame with the specified columns\n",
    "    df = pd.DataFrame(columns=LIST_VARIABLES)\n",
    "    # we fix types so that we can safely initialize using pd.NA\n",
    "    df = df.astype({\n",
    "        col: (\n",
    "            pd.StringDtype() if col in TYPE_STRING else\n",
    "            pd.Int64Dtype() if col in TYPE_INTEGER else\n",
    "            pd.CategoricalDtype() if col in TYPE_CATEGORICAL else\n",
    "            pd.Float64Dtype()\n",
    "        ) for col in df.columns\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "# Constructor for a new row in the data frame\n",
    "# Initializes a new row with pd.NA for all variables\n",
    "# except the hour variable which is set to the given hour\n",
    "# Input:    - hour = timestamp associated with the new row\n",
    "# Output:   - a pandas.DataFrame new_row with a single row\n",
    "#             initialized with values pd.NA and in the same\n",
    "#             format as the output of initialize DataFrame for\n",
    "#             concatenation.\n",
    "def initializeNewRow(hour):\n",
    "    # Create a new row with pd.NA for all variables\n",
    "    new_row = pd.DataFrame(\n",
    "        [[pd.NA] * len(LIST_VARIABLES)],\n",
    "        columns=LIST_VARIABLES\n",
    "    )\n",
    "    # we fix types so that we can safely initialize using pd.NA\n",
    "    new_row = new_row.astype({\n",
    "        col: (\n",
    "            pd.StringDtype() if col in TYPE_STRING else\n",
    "            pd.Int64Dtype() if col in TYPE_INTEGER else\n",
    "            pd.CategoricalDtype() if col in TYPE_CATEGORICAL else\n",
    "            pd.Float64Dtype()\n",
    "        ) for col in new_row.columns\n",
    "    })\n",
    "    new_row['Hour'] = hour\n",
    "    return new_row\n",
    "\n",
    "\n",
    "# We define a custom exception for data format errors; cf. preprocessLine\n",
    "# This is used to catch cases where the input data is not provided\n",
    "# in the expected format.\n",
    "class DataFormatException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Supposedly every line in a data file has three comma-separated values:\n",
    "#   - a time stamp in format hh:mm\n",
    "#   - a variable name\n",
    "#   - a value for the variable\n",
    "# This function checks the format of a line and returns the split values. It\n",
    "# raises a DataFormatException if the line does not contain exactly three\n",
    "# values\n",
    "# Input:    - A string\n",
    "# Output:   - A list of three strings:\n",
    "#             [timeStamp, variableName, value]\n",
    "def preprocessLine(line):\n",
    "    lineSplit = line.strip().split(\",\")\n",
    "    if len(lineSplit) != 3:\n",
    "        raise DataFormatException()\n",
    "    return lineSplit\n",
    "\n",
    "\n",
    "# extract the hour value from a time string\n",
    "# Input:    - timeString in format hh:mm\n",
    "# Output:   - hour as an integer, rounded up if minute > 0\n",
    "def processTimeStamp(timeString):\n",
    "    # timeString is in format hh:mm\n",
    "    hour = int(timeString.split(\":\")[0])\n",
    "    minute = int(timeString.split(\":\")[1])\n",
    "    # normalize to hour, rounding up\n",
    "    if minute > 00:\n",
    "        hour += 1\n",
    "    return hour\n",
    "\n",
    "\n",
    "# We build the data frame in long format using one row per hour.\n",
    "# Variables might not be available for all hours and we also allow\n",
    "# the input data to be unordered.\n",
    "# This function expands the data frame to include all missing hours\n",
    "# between the last timestamp and the current hour.\n",
    "# Input:    - hour = the current hour to be processed\n",
    "#           - timestamp = the last processed timeStamp\n",
    "#           - df = the data frame to be expanded\n",
    "# Output:   - the expanded data frame with all hours from the last\n",
    "def expandDataFrame(hour, timestamp, df):\n",
    "    while timestamp < hour:\n",
    "        # Check if the hour is already in the data frame\n",
    "        condition = not (df['Hour'] == hour).any()\n",
    "        if condition:\n",
    "            timestamp += 1\n",
    "            if not (df['Hour'] == timestamp).any():\n",
    "                # Create a new row for the current timestamp\n",
    "                new_row = initializeNewRow(timestamp)\n",
    "                # Append the new row to the data frame\n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# create a data frame per patient\n",
    "# Input:    - Path to an individual record's data\n",
    "# Output:   - a pandas.DataFrame with the time series data for this individual\n",
    "#             in long format with one row per hour\n",
    "def patientDataFrame(rawDataPath):\n",
    "    df = initializeDataFrame()\n",
    "    data = open(rawDataPath)\n",
    "    # first line is a header\n",
    "    next(data)\n",
    "    # read the remainder of the file line-by-line\n",
    "    timestamp = -1\n",
    "    # We first save the static variables in a dict\n",
    "    # and fill up the rows later\n",
    "    staticDict = initializeStaticDict()\n",
    "    # now we parse the file line by line\n",
    "    fileName = os.path.basename(rawDataPath)\n",
    "    RecordID = \"\"\n",
    "    # we keep track of the line number for logging purposes\n",
    "    lineNumber = 1  # we ignore the header\n",
    "    for line in data:\n",
    "        lineNumber += 1\n",
    "        try:\n",
    "            lineSplit = preprocessLine(line)\n",
    "            hour = processTimeStamp(lineSplit[0])\n",
    "            df = expandDataFrame(hour, timestamp, df)\n",
    "            timestamp = hour\n",
    "            if lineSplit[1] in staticDict.keys():\n",
    "                # This is a static variable, save it\n",
    "                staticDict[lineSplit[1]] = lineSplit[2]\n",
    "            elif lineSplit[1] == \"RecordID\":\n",
    "                # This is the RecordID, save it\n",
    "                RecordID = lineSplit[2]\n",
    "            else:\n",
    "                # This is a time-series variable, save it\n",
    "                try:\n",
    "                    value = float(lineSplit[2])\n",
    "                except ValueError:\n",
    "                    # If the value cannot be converted to float, skip it\n",
    "                    message = (\n",
    "                        f\"Value in file {fileName} in line {lineNumber}\"\n",
    "                        f\" cannot be converted to float: {lineSplit[2]}\"\n",
    "                    )\n",
    "                    logger.warning(message)\n",
    "                    continue\n",
    "                # add the value to the data frame\n",
    "                # Note: some values might have multiple observations within\n",
    "                # one hour; we take the last one since we don't have enough\n",
    "                # information to decide how to aggregate them\n",
    "                df.loc[df[\"Hour\"] == hour, lineSplit[1]] = value\n",
    "        except DataFormatException:\n",
    "            message = (\n",
    "                f\"Data format error in file {fileName}\"\n",
    "                f\" in line {lineNumber}\"\n",
    "            )\n",
    "            logger.warning(message)\n",
    "            pass\n",
    "    # add static variables\n",
    "    for key, value in staticDict.items():\n",
    "        df[key] = value\n",
    "        # add RecordID and FileName\n",
    "        df['RecordID'] = RecordID\n",
    "    return df\n",
    "\n",
    "\n",
    "# Create df in long format for each folder in dataPaths\n",
    "# It is assumed that each folder contains multiple files, one txt\n",
    "# file per patient, containing all the measured variables.\n",
    "# Input:    - dataPath = path to the folder containing the patient\n",
    "# Output:   - a pandas.DataFrame with the time series data for all patients\n",
    "#             in dataPath using long format with one row per hour\n",
    "def rawDataToLongFormat(rawDataPath):\n",
    "    df = initializeDataFrame()\n",
    "    for file in tqdm(os.listdir(rawDataPath)):\n",
    "        if file.endswith(\".txt\"):\n",
    "            # Create a data frame for the patient\n",
    "            patient_df = patientDataFrame(os.path.join(rawDataPath, file))\n",
    "            # Add the patient data frame to the main data frame\n",
    "            df = pd.concat([df, patient_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# We process an outcomes file.\n",
    "# Essentially, we import the data as a pandas data frame, which we return.\n",
    "# If test=True, then we only keep the outcomes corresponding to patients\n",
    "# retained for the test sample. In particular, this uses the processed\n",
    "# input data as produced by rawDataToLongFormat\n",
    "# Input:    - rawOutcomesPath = Path to a file containing the original\n",
    "#             outcome data\n",
    "#           - dataLongFormat = data frame as returned from rawDataToLongFormat\n",
    "def processOutcomes(rawOutcomesPath, dataLongFormat):\n",
    "    relevantRecords = dataLongFormat['RecordID'].unique()\n",
    "    dfOut = pd.read_csv(rawOutcomesPath)\n",
    "    # if test, we need to drop some entries\n",
    "    if test:\n",
    "        # for matching, we need to use RecordID as a string (which is the type\n",
    "        # used in dataLongFormat)\n",
    "        dfOut['RecordID'] = dfOut['RecordID'].astype(str)\n",
    "        # we only keep the relevant records\n",
    "        drop_indices = dfOut[~dfOut['RecordID'].isin(relevantRecords)].index\n",
    "        dfOut.drop(index=drop_indices, inplace=True)\n",
    "        if dfOut.empty:\n",
    "            logging.warning(\n",
    "                f\"Processing of {rawOutcomesPath} resulted \"\n",
    "                \"in an empty data frame.\"\n",
    "            )\n",
    "    return dfOut\n",
    "\n",
    "\n",
    "# Before applying processOutcomeData, we need to determine the rawOutcomesPath\n",
    "# corresponding to a given rawDataPath\n",
    "# the following two functions do exactly this\n",
    "# Input:    - rawDataPath = Path to the original input data being processed\n",
    "# Output:   - key = 'a', 'b', 'c' indicating training, validation, or test data\n",
    "def extractDataKey(rawDataPath):\n",
    "    # Extract 'VAL', e.g., 'a', 'b' 'c', from 'set-VAL' or 'set-VAL-test'\n",
    "    basename = os.path.basename(rawDataPath)\n",
    "    key = re.match(r'set-([^/-]+)(?:-test)?$', basename).group(1)\n",
    "    return key\n",
    "\n",
    "\n",
    "# Input:    - rawDataPath = Path to the original input data being processed\n",
    "# Output:   - rawOutcomesPath = Path to the original file containing the\n",
    "#             outcomes corresponding to the input path\n",
    "def findOutcomes(rawDataPath):\n",
    "    key = extractDataKey(rawDataPath)\n",
    "    for rawOutcomePath in rawOutcomesPaths:\n",
    "        basename = os.path.basename(rawOutcomePath)\n",
    "        # Extract letter before .txt in Outcomes-?.txt\n",
    "        m = re.match(r'Outcomes-([a-zA-Z])\\.txt$', basename)\n",
    "        if m and m.group(1) == key:\n",
    "            return rawOutcomePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4da8c0-d225-4ed4-a774-3628f8c9b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder data in the root directory, which contains the\n",
    "# data we actually work with. In particular, the time series data in long\n",
    "# format, stored as parquet.gzip and the outcome data\n",
    "try:\n",
    "    os.mkdir(DATA_DIR)\n",
    "except FileExistsError:\n",
    "    logging.info(\n",
    "        f\"{os.path.basename(DATA_DIR)} exists and won't be created\"\n",
    "    )\n",
    "    pass\n",
    "\n",
    "# Transform data to frames in long format and save as parquet\n",
    "# We produce one data frame per set according to the split into\n",
    "# training, validation, and testing\n",
    "for rawDataPath in rawDataPaths:\n",
    "    logger.info(\n",
    "        \"Processing raw data in \"\n",
    "        f\"{os.path.basename(os.path.normpath(rawDataPath))}...\"\n",
    "    )\n",
    "    # Create the long format data frame\n",
    "    df = rawDataToLongFormat(rawDataPath)\n",
    "    # we define the file name, e.g., set-a.parquet.gzip, independent\n",
    "    # of whether test = True or test = False\n",
    "    rawDataName = os.path.basename(rawDataPath)\n",
    "    name = re.search(r'([^/]+?)(?:-test)?$', rawDataName).group(1)\n",
    "    outDataName = f\"{name}.parquet.gzip\"\n",
    "    outDataPath = os.path.join(DATA_DIR, outDataName)\n",
    "    # save long format data to parquet\n",
    "    df.to_parquet(outDataPath, index=False)\n",
    "    logger.info(\n",
    "        f\"Saved {outDataName} to {os.path.basename(DATA_DIR)}.\"\n",
    "    )\n",
    "    # We process the corresponding outcomes\n",
    "    rawOutcomesPath = findOutcomes(rawDataPath)\n",
    "    dfOutcomes = processOutcomes(rawOutcomesPath, df)\n",
    "    # save outcomes data to parquet\n",
    "    outOutcomesName = f\"{name}-outcomes.parquet.gzip\"\n",
    "    outOutcomesPath = os.path.join(DATA_DIR, outOutcomesName)\n",
    "    dfOutcomes.to_parquet(outOutcomesPath, index=False)\n",
    "    logger.info(\n",
    "        f\"Saved {outOutcomesName} to {os.path.basename(DATA_DIR)}.\"\n",
    "    )\n",
    "\n",
    "print(\"Data preparation completed successfully.\")\n",
    "logger.info(\"Data preparation completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
