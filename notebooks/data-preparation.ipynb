{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d886b04-868a-46d1-9b7e-a7f4945fb1af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Tasks performed in this notebook\n",
    "* Read raw data\n",
    "* Transform to time-series data per individual in long format\n",
    "* Normalize time-points to hourly values\n",
    "* Save time-series data in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a15a52-d5ec-4589-97db-11991e5a67fc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d67d1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logger = logging.getLogger('data-preparation-logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if logger.hasHandlers():\n",
    "    # If the logger already has handlers, remove them\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "\n",
    "if not logger.hasHandlers():\n",
    "    # Create a directory for logs if it doesn't exist\n",
    "    log_dir = os.path.join(os.path.dirname(os.getcwd()), 'logs')\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Create a file handler\n",
    "    log_file = os.path.join(log_dir, 'data-preparation.log')\n",
    "    fh = logging.FileHandler(log_file, mode='w')\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Create a console handler\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.CRITICAL)\n",
    "\n",
    "    # Create a formatter and set it for both handlers\n",
    "    log_format = '%(asctime)s - %(name)s - %(levelname)s:\\n\\t\\t%(message)s'\n",
    "    formatter = logging.Formatter(log_format)\n",
    "    fh.setFormatter(formatter)\n",
    "    ch.setFormatter(formatter)\n",
    "\n",
    "    # Add the handlers to the logger\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951bc19-6012-4922-ac31-dc376fc1ddee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Preperations:\n",
    "# Boolean determining whether the script is run on the full data or\n",
    "# on a subset residing in subfolders with suffix -test\n",
    "# If the testdata doesn't exist, it will be created.\n",
    "# This is for debugging purposes only. The test data created is platform\n",
    "# dependent because of the use of os.scandir(), hence the results\n",
    "# are not reproducible.\n",
    "test = True\n",
    "\n",
    "# A list of all the time series variables to consider\n",
    "LIST_VARIABLE_TS = [\n",
    "    'ALP', 'ALT', 'AST', 'Albumin', 'BUN', 'Bilirubin',\n",
    "    'Cholesterol', 'Creatinine', 'DiasABP', 'FiO2', 'GCS', 'Glucose',\n",
    "    'HCO3', 'HCT', 'HR', 'ICUType', 'K', 'Lactate', 'MAP', 'MechVent',\n",
    "    'Mg', 'NIDiasABP', 'NIMAP', 'NISysABP', 'Na', 'PaCO2', 'PaO2', 'Platelets',\n",
    "    'RespRate', 'SaO2', 'SysABP', 'Temp', 'TroponinI', 'TroponinT', 'Urine',\n",
    "    'WBC', 'pH'\n",
    "]\n",
    "\n",
    "# A list of static variables\n",
    "LIST_VARIABLE_STATIC = [\n",
    "    'Age', 'Gender', 'Height', 'Weight'\n",
    "]\n",
    "\n",
    "# A list of keys:\n",
    "# RecordID indexes the individual times series\n",
    "# Hour is the time stamp to be used -- note that we will have to normalize\n",
    "# the variable Time which is in format hh:mm after admission to hourly\n",
    "# variables\n",
    "LIST_VARIABLE_KEYS = [\n",
    "    'RecordID', 'Hour'\n",
    "]\n",
    "\n",
    "# A list of all variables, including time series and static variables\n",
    "LIST_VARIABLES = LIST_VARIABLE_KEYS + LIST_VARIABLE_STATIC + LIST_VARIABLE_TS\n",
    "\n",
    "# fix types for the different variables\n",
    "TYPE_STRING = ['RecordID']\n",
    "TYPE_INTEGER = ['Hour']\n",
    "TYPE_CATEGORICAL = ['Gender']\n",
    "TYPE_FLOAT = (LIST_VARIABLE_TS + LIST_VARIABLE_STATIC).remove('Gender')\n",
    "\n",
    "# Fix a threshold for zero\n",
    "LOG_ZERO_OFFSET = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f15cb-b761-4186-a5a4-c36c36360f07",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Fix path for data\n",
    "DATA_PATH = os.path.join(os.path.dirname(os.getcwd()),\n",
    "                         \"physionet.org/files/challenge-2012/1.0.0/\")\n",
    "DATA_TRAINING = os.path.join(DATA_PATH, \"set-a\")\n",
    "DATA_VALIDATION = os.path.join(DATA_PATH, \"set-b\")\n",
    "DATA_TESTING = os.path.join(DATA_PATH, \"set-c\")\n",
    "dataPaths = [DATA_TRAINING, DATA_VALIDATION, DATA_TESTING]\n",
    "\n",
    "# if we want to run the code on a test subset, generate test data\n",
    "if test:\n",
    "    for dataPath in dataPaths:\n",
    "        try:\n",
    "            newFolder = dataPath + \"-test\"\n",
    "            os.mkdir(newFolder)\n",
    "            i = 0\n",
    "            for entry in os.scandir(dataPath):\n",
    "                if i < 100 and entry.name.endswith(\".txt\"):\n",
    "                    i += 1\n",
    "                    newPath = os.path.join(newFolder, entry.name)\n",
    "                    shutil.copy(entry.path, newPath)\n",
    "        except FileExistsError:\n",
    "            logger.info(\n",
    "                \"Test data folder \"\n",
    "                f\"{os.path.basename(os.path.normpath(dataPath))}-test \"\n",
    "                \"already exists. Using existing data.\"\n",
    "            )\n",
    "            pass\n",
    "    dataPaths = [p + \"-test\" for p in dataPaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the functions used for the data transformation\n",
    "\n",
    "\n",
    "# Constructor for the static dictionary:\n",
    "# We produce a long format data frame per patient, where the static variables\n",
    "# are treated as constants. Hence we keep track of them while producing the\n",
    "# data frame and fill them in the end.\n",
    "# - keys are the names of the static variables\n",
    "# - initialized with values pd.NA\n",
    "# Input:    - None\n",
    "# Output:   - a dictionary with keys as defined in LIST_VARIABLE_STATIC\n",
    "def initializeStaticDict():\n",
    "    staticDict = {key: pd.NA for key in LIST_VARIABLE_STATIC}\n",
    "    return staticDict\n",
    "\n",
    "\n",
    "# Constructor for the patient data frame\n",
    "# Initializes an empty data frame with a column per variable as defined before\n",
    "# Input:    - None\n",
    "# Output:   - a pandas.DataFrame with columns according to LIST_VARIABLES,\n",
    "#             i.e., the list of variables defined above.\n",
    "def initializeDataFrame():\n",
    "    # Create an empty DataFrame with the specified columns\n",
    "    df = pd.DataFrame(columns=LIST_VARIABLES)\n",
    "    # we fix types so that we can safely initialize using pd.NA\n",
    "    df = df.astype({\n",
    "        col: (\n",
    "            pd.StringDtype() if col in TYPE_STRING else\n",
    "            pd.Int64Dtype() if col in TYPE_INTEGER else\n",
    "            pd.CategoricalDtype() if col in TYPE_CATEGORICAL else\n",
    "            pd.Float64Dtype()\n",
    "        ) for col in df.columns\n",
    "    })\n",
    "    return df\n",
    "\n",
    "\n",
    "# Constructor for a new row in the data frame\n",
    "# Initializes a new row with pd.NA for all variables\n",
    "# except the hour variable which is set to the given hour\n",
    "# Input:    - hour = timestamp associated with the new row\n",
    "# Output:   - a pandas.DataFrame new_row with a single row\n",
    "#             initialized with values pd.NA and in the same\n",
    "#             format as the output of initialize DataFrame for\n",
    "#             concatenation.\n",
    "def initializeNewRow(hour):\n",
    "    # Create a new row with pd.NA for all variables\n",
    "    new_row = pd.DataFrame(\n",
    "        [[pd.NA] * len(LIST_VARIABLES)],\n",
    "        columns=LIST_VARIABLES\n",
    "    )\n",
    "    # we fix types so that we can safely initialize using pd.NA\n",
    "    new_row = new_row.astype({\n",
    "        col: (\n",
    "            pd.StringDtype() if col in TYPE_STRING else\n",
    "            pd.Int64Dtype() if col in TYPE_INTEGER else\n",
    "            pd.CategoricalDtype() if col in TYPE_CATEGORICAL else\n",
    "            pd.Float64Dtype()\n",
    "        ) for col in new_row.columns\n",
    "    })\n",
    "    new_row['Hour'] = hour\n",
    "    return new_row\n",
    "\n",
    "\n",
    "# We define a custom exception for data format errors; cf. preprocessLine\n",
    "# This is used to catch cases where the input data is not provided\n",
    "# in the expected format.\n",
    "class DataFormatException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Supposedly every line in a data file has three comma-separated values:\n",
    "#   - a time stamp in format hh:mm\n",
    "#   - a variable name\n",
    "#   - a value for the variable\n",
    "# This function checks the format of a line and returns the split values. It\n",
    "# raises a DataFormatException if the line does not contain exactly three\n",
    "# values\n",
    "# Input:    - A string\n",
    "# Output:   - A list of three strings:\n",
    "#             [timeStamp, variableName, value]\n",
    "def preprocessLine(line):\n",
    "    lineSplit = line.strip().split(\",\")\n",
    "    if len(lineSplit) != 3:\n",
    "        raise DataFormatException()\n",
    "    return lineSplit\n",
    "\n",
    "\n",
    "# extract the hour value from a time string\n",
    "# Input:    - timeString in format hh:mm\n",
    "# Output:   - hour as an integer, rounded up if minute > 0\n",
    "def processTimeStamp(timeString):\n",
    "    # timeString is in format hh:mm\n",
    "    hour = int(timeString.split(\":\")[0])\n",
    "    minute = int(timeString.split(\":\")[1])\n",
    "    # normalize to hour, rounding up\n",
    "    if minute > 00:\n",
    "        hour += 1\n",
    "    return hour\n",
    "\n",
    "\n",
    "# We build the data frame in long format using one row per hour.\n",
    "# Variables might not be available for all hours and we also allow\n",
    "# the input data to be unordered.\n",
    "# This function expands the data frame to include all missing hours\n",
    "# between the last timestamp and the current hour.\n",
    "# Input:    - hour = the current hour to be processed\n",
    "#           - timestamp = the last processed timeStamp\n",
    "#           - df = the data frame to be expanded\n",
    "# Output:   - the expanded data frame with all hours from the last\n",
    "def expandDataFrame(hour, timestamp, df):\n",
    "    while timestamp < hour:\n",
    "        # Check if the hour is already in the data frame\n",
    "        condition = not (df['Hour'] == hour).any()\n",
    "        if condition:\n",
    "            timestamp += 1\n",
    "            if not (df['Hour'] == timestamp).any():\n",
    "                # Create a new row for the current timestamp\n",
    "                new_row = initializeNewRow(timestamp)\n",
    "                # Append the new row to the data frame\n",
    "                df = pd.concat([df, new_row], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# create a data frame per patient\n",
    "# Input:    - Path to an individual record's data\n",
    "# Output:   - a pandas.DataFrame with the time series data for this individual\n",
    "#             in long format with one row per hour\n",
    "def patientDataFrame(dataPath):\n",
    "    df = initializeDataFrame()\n",
    "    data = open(dataPath)\n",
    "    # first line is a header\n",
    "    next(data)\n",
    "    # read the remainder of the file line-by-line\n",
    "    timestamp = -1\n",
    "    # We first save the static variables in a dict\n",
    "    # and fill up the rows later\n",
    "    staticDict = initializeStaticDict()\n",
    "    # now we parse the file line by line\n",
    "    fileName = os.path.basename(dataPath)\n",
    "    RecordID = \"\"\n",
    "    # we keep track of the line number for logging purposes\n",
    "    lineNumber = 1  # we ignore the header\n",
    "    for line in data:\n",
    "        lineNumber += 1\n",
    "        try:\n",
    "            lineSplit = preprocessLine(line)\n",
    "            hour = processTimeStamp(lineSplit[0])\n",
    "            df = expandDataFrame(hour, timestamp, df)\n",
    "            timestamp = hour\n",
    "            if lineSplit[1] in staticDict.keys():\n",
    "                # This is a static variable, save it\n",
    "                staticDict[lineSplit[1]] = lineSplit[2]\n",
    "            elif lineSplit[1] == \"RecordID\":\n",
    "                # This is the RecordID, save it\n",
    "                RecordID = lineSplit[2]\n",
    "            else:\n",
    "                # This is a time-series variable, save it\n",
    "                try:\n",
    "                    value = float(lineSplit[2])\n",
    "                except ValueError:\n",
    "                    # If the value cannot be converted to float, skip it\n",
    "                    message = (\n",
    "                        f\"Value in file {fileName} in line {lineNumber}\"\n",
    "                        f\" cannot be converted to float: {lineSplit[2]}\"\n",
    "                    )\n",
    "                    logger.warning(message)\n",
    "                    continue\n",
    "                # add the value to the data frame\n",
    "                # Note: some values might have multiple observations within\n",
    "                # one hour; we take the last one since we don't have enough\n",
    "                # information to decide how to aggregate them\n",
    "                df.loc[df[\"Hour\"] == hour, lineSplit[1]] = value\n",
    "        except DataFormatException:\n",
    "            message = (\n",
    "                f\"Data format error in file {fileName}\"\n",
    "                f\" in line {lineNumber}\"\n",
    "            )\n",
    "            logger.warning(message)\n",
    "            pass\n",
    "    # add static variables\n",
    "    for key, value in staticDict.items():\n",
    "        df[key] = value\n",
    "        # add RecordID and FileName\n",
    "        df['RecordID'] = RecordID\n",
    "    return df\n",
    "\n",
    "\n",
    "# Create df in long format for each folder in dataPaths\n",
    "# It is assumed that each folder contains multiple files, one txt\n",
    "# file per patient, containing all the measured variables.\n",
    "# Input:    - dataPath = path to the folder containing the patient\n",
    "# Output:   - a pandas.DataFrame with the time series data for all patients\n",
    "#             in dataPath using long format with one row per hour\n",
    "def dataToLongFormat(dataPath):\n",
    "    df = initializeDataFrame()\n",
    "    for file in tqdm(os.listdir(dataPath)):\n",
    "        if file.endswith(\".txt\"):\n",
    "            # Create a data frame for the patient\n",
    "            patient_df = patientDataFrame(os.path.join(dataPath, file))\n",
    "            # Add the patient data frame to the main data frame\n",
    "            df = pd.concat([df, patient_df], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4da8c0-d225-4ed4-a774-3628f8c9b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to frames in long format and save as parquet\n",
    "# We produce one data frame per set according to the split into\n",
    "# training, validation, and testing\n",
    "for dataPath in dataPaths:\n",
    "    logger.info(\n",
    "        \"Processing data in \"\n",
    "        f\"{os.path.basename(os.path.normpath(dataPath))}...\"\n",
    "    )\n",
    "    # Create the long format data frame\n",
    "    df = dataToLongFormat(dataPath)\n",
    "    # save data to parquet format\n",
    "    outputFileName = os.path.basename(dataPath) + \"_long_format.parquet.gzip\"\n",
    "    outputFilePath = os.path.join(dataPath, outputFileName)\n",
    "    df.to_parquet(outputFilePath, index=False)\n",
    "    logger.info(f\"Saved {outputFileName} with {len(df)} rows.\")\n",
    "print(\"Data preparation completed successfully.\")\n",
    "logger.info(\"Data preparation completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e752f4-285f-4c74-ad73-4989b74f3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"5\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
